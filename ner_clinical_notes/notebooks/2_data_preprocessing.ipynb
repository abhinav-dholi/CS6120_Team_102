{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clinical NER Data Preprocessing\n",
    "\n",
    "This notebook processes the MACCROBAT2018 dataset and converts it from JSON format to BIO format for NER training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/abhinavdholi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/abhinavdholi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import shutil\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "data_dir = \"../data\"\n",
    "raw_data_dir = os.path.join(data_dir, \"raw\")\n",
    "processed_data_dir = os.path.join(data_dir, \"processed\")\n",
    "\n",
    "os.makedirs(raw_data_dir, exist_ok=True)\n",
    "os.makedirs(processed_data_dir, exist_ok=True)\n",
    "os.makedirs(os.path.join(processed_data_dir, \"bio_data\"), exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity Type Mapping\n",
    "\n",
    "Define mapping between entity types and their acronyms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_to_acronyms = {\n",
    "    'Activity': 'ACT',\n",
    "    'Administration': 'ADM',\n",
    "    'Age': 'AGE',\n",
    "    'Area': 'ARA',\n",
    "    'Biological_attribute': 'BAT',\n",
    "    'Biological_structure': 'BST',\n",
    "    'Clinical_event': 'CLE',\n",
    "    'Color': 'COL',\n",
    "    'Coreference': 'COR',\n",
    "    'Date': 'DAT',\n",
    "    'Detailed_description': 'DET',\n",
    "    'Diagnostic_procedure': 'DIA',\n",
    "    'Disease_disorder': 'DIS',\n",
    "    'Distance': 'DIS',\n",
    "    'Dosage': 'DOS',\n",
    "    'Duration': 'DUR',\n",
    "    'Family_history': 'FAM',\n",
    "    'Frequency': 'FRE',\n",
    "    'Height': 'HEI',\n",
    "    'History': 'HIS',\n",
    "    'Lab_value': 'LAB',\n",
    "    'Mass': 'MAS',\n",
    "    'Medication': 'MED',\n",
    "    'Nonbiological_location': 'NBL',\n",
    "    'Occupation': 'OCC',\n",
    "    'Other_entity': 'OTH',\n",
    "    'Other_event': 'OTE',\n",
    "    'Outcome': 'OUT',\n",
    "    'Personal_background': 'PER',\n",
    "    'Qualitative_concept': 'QUC',\n",
    "    'Quantitative_concept': 'QUC',\n",
    "    'Severity': 'SEV',\n",
    "    'Sex': 'SEX',\n",
    "    'Shape': 'SHA',\n",
    "    'Sign_symptom': 'SIG',\n",
    "    'Subject': 'SUB',\n",
    "    'Texture': 'TEX',\n",
    "    'Therapeutic_procedure': 'THP',\n",
    "    'Time': 'TIM',\n",
    "    'Volume': 'VOL',\n",
    "    'Weight': 'WEI'\n",
    "}\n",
    "\n",
    "acronyms_to_entities = {v: k for k, v in entity_to_acronyms.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully with 200 documents.\n",
      "Saved parsed data to ../data/raw/annotated_data.json\n"
     ]
    }
   ],
   "source": [
    "def parse_ann_file(ann_file_path):\n",
    "    \"\"\"\n",
    "    Parses an annotation file in the BRAT format.\n",
    "    \"\"\"\n",
    "    annotations = []\n",
    "    with open(ann_file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if line.strip() and line.startswith('T'):  # entity annotations start with T\n",
    "                parts = line.strip().split('\\t')\n",
    "                if len(parts) >= 3:\n",
    "                    ann_id = parts[0]\n",
    "                    ann_info = parts[1].split()\n",
    "                    \n",
    "                    label = ann_info[0]\n",
    "                    start = int(ann_info[1])\n",
    "                    end = int(ann_info[-1])\n",
    "                    \n",
    "                    annotations.append({\n",
    "                        'label': label,\n",
    "                        'start': start,\n",
    "                        'end': end\n",
    "                    })\n",
    "    return annotations\n",
    "\n",
    "def load_maccrobat_data(raw_data_dir):\n",
    "    \"\"\"\n",
    "    Loads MACCROBAT dataset from .txt and .ann files.\n",
    "    \"\"\"\n",
    "    data = {}\n",
    "    \n",
    "    # gets all .txt files\n",
    "    txt_files = [f for f in os.listdir(raw_data_dir) if f.endswith('.txt')]\n",
    "    \n",
    "    for txt_file in txt_files:\n",
    "        doc_id = txt_file.split('.')[0]  # removes .txt extension to get document ID\n",
    "        ann_file = f\"{doc_id}.ann\"\n",
    "        \n",
    "        # checks if corresponding .ann file exists\n",
    "        if not os.path.exists(os.path.join(raw_data_dir, ann_file)):\n",
    "            print(f\"Warning: No annotation file found for {txt_file}\")\n",
    "            continue\n",
    "        \n",
    "        # reads text file\n",
    "        with open(os.path.join(raw_data_dir, txt_file), 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "        \n",
    "        # parses annotations\n",
    "        annotations = parse_ann_file(os.path.join(raw_data_dir, ann_file))\n",
    "        \n",
    "        # adds to dataset\n",
    "        data[doc_id] = {\n",
    "            'text': text,\n",
    "            'annotations': annotations\n",
    "        }\n",
    "    \n",
    "    return data\n",
    "\n",
    "try:\n",
    "    data = load_maccrobat_data(os.path.join(raw_data_dir, \"MACCROBAT2018\"))\n",
    "    print(f\"Data loaded successfully with {len(data)} documents.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {e}\")\n",
    "    data = {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "Load the JSON data file containing the annotated clinical notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully with 200 documents.\n"
     ]
    }
   ],
   "source": [
    "# Loads the JSON data\n",
    "# If you don't have the file, you need to place it in the raw_data_dir\n",
    "json_file_path = os.path.join(raw_data_dir, \"annotated_data.json\")\n",
    "\n",
    "try:\n",
    "    with open(json_file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    print(f\"Data loaded successfully with {len(data)} documents.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file {json_file_path} was not found.\")\n",
    "    print(\"Please ensure you have the MACCROBAT2018 dataset in the raw data directory.\")\n",
    "    data = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Processing Functions\n",
    "\n",
    "Define functions for processing text and creating BIO tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_trailing_punctuation(token):\n",
    "    \"\"\"\n",
    "    Removes trailing punctuation from a token.\n",
    "    \"\"\"\n",
    "    while token and re.search(r'[^\\w\\s\\']', token[-1]):\n",
    "        token = token[:-1]\n",
    "        \n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(text):\n",
    "    \"\"\"\n",
    "    Split text into tokens, start-end ranges, and sentence breaks.\n",
    "    \"\"\"\n",
    "    regex_match = r'[^\\s\\u200a\\-\\u2010-\\u2015\\u2212\\uff0d]+'\n",
    "    \n",
    "    tokens = []\n",
    "    start_end_ranges = []\n",
    "    sentence_breaks = []\n",
    "    \n",
    "    start_idx = 0\n",
    "    \n",
    "    for sentence in text.split('\\n'):\n",
    "        words = [match.group(0) for match in re.finditer(regex_match, sentence)]\n",
    "        processed_words = list(map(remove_trailing_punctuation, words))\n",
    "        sentence_indices = [(match.start(), match.start() + len(token)) for match, token in\n",
    "                          zip(re.finditer(regex_match, sentence), processed_words)]\n",
    "        \n",
    "        # updates the indices to account for the current sentence's position in the entire text\n",
    "        sentence_indices = [(start_idx + start, start_idx + end) for start, end in sentence_indices]\n",
    "        \n",
    "        start_end_ranges.extend(sentence_indices)\n",
    "        tokens.extend(processed_words)\n",
    "        \n",
    "        sentence_breaks.append(len(tokens))\n",
    "        \n",
    "        start_idx += len(sentence) + 1\n",
    "    return tokens, start_end_ranges, sentence_breaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_token(tokens, tags, token_pos, entity):\n",
    "    \"\"\"\n",
    "    Assign BIO tag to a token based on its position and entity type.\n",
    "    \"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    tag = entity_to_acronyms[entity]\n",
    "    \n",
    "    if token_pos > 0 and f'{tag}' in tags[token_pos - 1]:\n",
    "        tags[token_pos] = f'I-{tag}'\n",
    "    elif tokens[token_pos].lower() not in stop_words:\n",
    "        tags[token_pos] = f'B-{tag}'\n",
    "    \n",
    "    return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_bio_files(output_file_path, tokens, tags, sentence_breaks):\n",
    "    \"\"\"\n",
    "    Write tokens and tags to a BIO format file.\n",
    "    \"\"\"\n",
    "    with open(output_file_path, 'w') as f:\n",
    "        for i in range(len(tokens)):\n",
    "            token = tokens[i].strip()\n",
    "            if token:\n",
    "                if i in sentence_breaks:\n",
    "                    f.write(\"\\n\")\n",
    "                f.write(f\"{tokens[i]}\\t{tags[i]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Annotations to BIO Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_ann_to_bio(data, output_dir, filtered_entities=[]):\n",
    "    \n",
    "    \"\"\"\n",
    "    Convert annotations from a dictionary of text files to a BIO-tagged sequence.\n",
    "    \"\"\"\n",
    "    \n",
    "    if os.path.exists(output_dir):\n",
    "        # deletes the contents of the directory\n",
    "        shutil.rmtree(output_dir)\n",
    "    # recreates the directory\n",
    "    os.makedirs(output_dir)\n",
    "    \n",
    "    \n",
    "    for file_id in data:\n",
    "        text = data[file_id]['text']\n",
    "        annotations = data[file_id]['annotations']\n",
    "        \n",
    "        # tokenizing\n",
    "        tokens, token2text, sentence_breaks = split_text(text)\n",
    "\n",
    "        # initializes the tags\n",
    "        tags = ['O'] * len(tokens)\n",
    "\n",
    "        ann_pos = 0\n",
    "        token_pos = 0\n",
    "\n",
    "        while ann_pos < len(annotations) and token_pos < len(tokens):\n",
    "\n",
    "            label = annotations[ann_pos]['label']\n",
    "            start = annotations[ann_pos]['start']\n",
    "            end = annotations[ann_pos]['end']\n",
    "\n",
    "            if filtered_entities:\n",
    "                if label not in filtered_entities:\n",
    "                    # increments to access next annotation\n",
    "                    ann_pos += 1\n",
    "                    continue\n",
    "            \n",
    "            ann_word = text[start:end]\n",
    "\n",
    "            # finds the next word that fall between the annotation start and end\n",
    "            while token_pos < len(tokens) and token2text[token_pos][0] < start:\n",
    "                \n",
    "                token_pos += 1\n",
    "\n",
    "            if tokens[token_pos] == ann_word or \\\n",
    "                ann_word in tokens[token_pos] or \\\n",
    "                re.sub(r'\\W+', '', ann_word) in re.sub(r'\\W+', '', tokens[token_pos]):\n",
    "                tag_token(tokens, tags, token_pos, label)\n",
    "            elif ann_word in tokens[token_pos - 1] or \\\n",
    "                ann_word in tokens[token_pos - 1] or \\\n",
    "                re.sub(r'\\W+', '', ann_word) in re.sub(r'\\W+', '', tokens[token_pos - 1]):\n",
    "                tag_token(tokens, tags, token_pos - 1, label)\n",
    "            else:\n",
    "                print(tokens[token_pos], tokens[token_pos - 1], ann_word, label)\n",
    "\n",
    "            # increments to access next annotation\n",
    "            ann_pos += 1\n",
    "\n",
    "        # writes to bio file\n",
    "        write_bio_files(os.path.join(output_dir, f\"{file_id}.bio\"), tokens, tags, sentence_breaks)\n",
    "    print(\"Conversion complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokens: ['Our', '24', 'year', 'old', 'non', 'smoking', 'male', 'patient', 'presented', 'with']\n",
      "Sample token positions: [(0, 3), (4, 6), (7, 11), (12, 15), (16, 19), (20, 27), (28, 32), (33, 40), (41, 50), (51, 55)]\n",
      "Sample annotations: [{'label': 'Age', 'start': 4, 'end': 6}, {'label': 'Age', 'start': 7, 'end': 11}, {'label': 'Age', 'start': 12, 'end': 15}]\n"
     ]
    }
   ],
   "source": [
    "# tests the conversion function on one sample document\n",
    "if data:\n",
    "    sample_doc_id = next(iter(data))\n",
    "    sample_doc = data[sample_doc_id]\n",
    "    tokens, token2text, sentence_breaks = split_text(sample_doc['text'][:200])\n",
    "    print(f\"Sample tokens: {tokens[:10]}\")\n",
    "    print(f\"Sample token positions: {token2text[:10]}\")\n",
    "    print(f\"Sample annotations: {sample_doc['annotations'][:3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion complete\n",
      "BIO files saved to ../data/processed/bio_data\n"
     ]
    }
   ],
   "source": [
    "# convert all documents to BIO format\n",
    "bio_output_dir = os.path.join(processed_data_dir, \"bio_data\")\n",
    "\n",
    "if data:\n",
    "    convert_ann_to_bio(data, bio_output_dir)\n",
    "    print(f\"BIO files saved to {bio_output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Converted Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample BIO file content:\n",
      "A\tO\n",
      "36\tB-AGE\n",
      "yr\tI-AGE\n",
      "old\tI-AGE\n",
      "previously\tB-HIS\n",
      "healthy\tI-HIS\n",
      "Sri\tB-PER\n",
      "Lankan\tI-PER\n",
      "male\tB-SEX\n",
      "who\tO\n",
      "takes\tB-OCC\n",
      "care\tI-OCC\n",
      "of\tI-OCC\n",
      "a\tI-OCC\n",
      "horse\tI-OCC\n",
      "presented\tB-CLE\n",
      "to\tO\n",
      "the\tO\n",
      "medical\tB-NBL\n",
      "casualty\tI-NBL\n",
      "ward\tI-NBL\n",
      "with\tO\n",
      "fever\tB-SIG\n",
      "arthralgia\tI-SIG\n",
      "and\tO\n",
      "myalgia\tB-SIG\n",
      "for\tO\n",
      "one\tB-DUR\n",
      "day\tI-DUR\n",
      "\n",
      "He\tO\n",
      "complained\tO\n",
      "of\tO\n",
      "mild\tB-SEV\n",
      "dysuria\tB-SIG\n",
      "but\tO\n",
      "had\tO\n",
      "normal\tB-LAB\n",
      "urine\tB-DIA\n",
      "output\tI-DIA\n",
      "\n",
      "He\tO\n",
      "did\tO\n",
      "not\tO\n",
      "have\tO\n",
      "chest\tB-BST\n",
      "pain\tB-SIG\n",
      "or\tO\n",
      "shortness\tB-SIG\n",
      "of\tI-SIG\n",
      "breath\tI-SIG\n",
      "\n",
      "Furth\n",
      "Total BIO files created: 200\n"
     ]
    }
   ],
   "source": [
    "# checks a sample BIO file\n",
    "import glob\n",
    "bio_output_dir = os.path.join(processed_data_dir, \"bio_data\")\n",
    "bio_files = glob.glob(os.path.join(bio_output_dir, \"*.bio\"))\n",
    "if bio_files:\n",
    "    with open(bio_files[0], 'r') as f:\n",
    "        sample_content = f.read(500)  # read first 500 characters\n",
    "    print(f\"Sample BIO file content:\\n{sample_content}\")\n",
    "    print(f\"Total BIO files created: {len(bio_files)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Entity Mappings\n",
    "\n",
    "Save entity mappings for later use in training and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity mappings saved to ../data/processed/entity_mappings.json\n"
     ]
    }
   ],
   "source": [
    "# saves entity mappings for later use\n",
    "mapping_file = os.path.join(processed_data_dir, \"entity_mappings.json\")\n",
    "with open(mapping_file, 'w') as f:\n",
    "    json.dump({\n",
    "        \"entity_to_acronyms\": entity_to_acronyms,\n",
    "        \"acronyms_to_entities\": acronyms_to_entities\n",
    "    }, f)\n",
    "\n",
    "print(f\"Entity mappings saved to {mapping_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we've:\n",
    "1. Set up the necessary directories\n",
    "2. Loaded the MACCROBAT2018 dataset\n",
    "3. Defined functions for processing text and creating BIO tags\n",
    "4. Converted the JSON annotations to BIO format\n",
    "5. Saved entity mappings for use in training and inference\n",
    "\n",
    "The processed data is now ready for model training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ner",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
